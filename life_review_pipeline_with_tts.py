# -*- coding: utf-8 -*-
"""life_review_pipeline_with_tts.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Uuo5y3GmM4pbLyQnlpqmvjU_SFxeUCK0

# Life Review AI Pipeline with Voice

This notebook extracts questions from PDFs, **speaks them aloud**, transcribes audio responses, and generates AI-based psychographic summaries.

## Text-to-Speech
- Questions are now spoken aloud using OpenAI's TTS
- Natural-sounding voice
- Auto-plays in the browser

## Setup Instructions
1. Add your OpenAI API key to Colab Secrets:
   - Click the üîë key icon in the left sidebar
   - Add a secret named `OPENAI_API_KEY`
   - Paste your API key as the value
2. Run all cells in order

## 1. Install Dependencies
"""

!pip install -q PyPDF2 openai

"""## 2. Import Libraries"""

import os
import PyPDF2
from openai import OpenAI
from google.colab import files, userdata
from google.colab.output import eval_js
from base64 import b64decode, b64encode
from IPython.display import Javascript, HTML, Audio, display
import time

"""## 3. Initialize OpenAI Client

This securely retrieves your API key from Colab Secrets.
"""

try:
    api_key = userdata.get('OPENAI_API_KEY')
    client = OpenAI(api_key=api_key)
    print("‚úÖ OpenAI client initialized successfully!")
except Exception as e:
    print(f"‚ùå Error: Could not retrieve API key from Colab Secrets.")
    print(f"Please add 'OPENAI_API_KEY' to Colab Secrets (üîë icon in left sidebar).")
    print(f"Error details: {e}")
    client = None

"""## 4. Define Functions"""

def extract_questions_from_pdf(pdf_path):
    """
    Reads a PDF and extracts all lines containing a question mark.

    Args:
        pdf_path (str): Path to the PDF file

    Returns:
        list: List of questions found in the PDF
    """
    questions = []
    try:
        with open(pdf_path, 'rb') as f:
            reader = PyPDF2.PdfReader(f)
            text = "".join(page.extract_text() or "" for page in reader.pages)

        # Extract lines containing questions
        for line in text.split("\n"):
            line = line.strip()
            if "?" in line and len(line) > 3:
                questions.append(line)

        if not questions:
            print("‚ö†Ô∏è No questions found in the PDF.")
        else:
            print(f"‚úÖ Extracted {len(questions)} question(s) from PDF.")

    except FileNotFoundError:
        print(f"‚ùå Error: File not found at {pdf_path}")
    except Exception as e:
        print(f"‚ùå Error reading PDF: {e}")

    return questions

def speak_text(text, voice="alloy"):
    """
    Converts text to speech using OpenAI TTS and plays it in the browser.

    Args:
        text (str): Text to speak
        voice (str): Voice to use (alloy, echo, fable, onyx, nova, shimmer)

    Returns:
        str: Path to the saved audio file or None if error
    """
    if not client:
        print("‚ùå OpenAI client not initialized. Cannot generate speech.")
        return None

    try:
        print(f"üîä Speaking: '{text[:50]}{'...' if len(text) > 50 else ''}'")

        # Generate speech with streaming
        response = client.audio.speech.create(
            model="tts-1",
            voice=voice,
            input=text
        )

        # Save to file
        speech_file = f"/content/question_speech_{int(time.time())}.mp3"

        with open(speech_file, 'wb') as f:
            for chunk in response.iter_bytes():
                f.write(chunk)

        # Auto-play in browser
        display(Audio(speech_file, autoplay=True))

        print("‚úÖ Speech generated and playing...")
        return speech_file

    except Exception as e:
        print(f"‚ùå Error generating speech: {e}")
        return None

def record_audio(duration_seconds=30):
    """
    Records audio directly in the browser using JavaScript with proper promise handling.

    Args:
        duration_seconds (int): Recording duration in seconds (default: 30)

    Returns:
        str: Path to the saved audio file or None if recording failed
    """
    print("üéôÔ∏è Click 'Allow' when prompted for microphone access.")
    print("üî¥ Recording will start automatically...")

    # JavaScript code for audio recording with countdown
    RECORD = """
    const sleep  = time => new Promise(resolve => setTimeout(resolve, time))
    const b2text = blob => new Promise(resolve => {
      const reader = new FileReader()
      reader.onloadend = e => resolve(e.srcElement.result)
      reader.readAsDataURL(blob)
    })

    var record = time => new Promise(async resolve => {
      // Create countdown display
      const countdownDiv = document.createElement('div');
      countdownDiv.style.cssText = 'font-size: 20px; font-weight: bold; color: #d32f2f; padding: 10px; background: #fff3e0; border-radius: 5px; margin: 10px 0;';
      const outputArea = document.querySelector('#output-area') || document.body;
      outputArea.appendChild(countdownDiv);

      try {
        stream = await navigator.mediaDevices.getUserMedia({ audio: true })
        recorder = new MediaRecorder(stream)
        chunks = []
        recorder.ondataavailable = e => chunks.push(e.data)
        recorder.start()

        // Countdown timer
        const totalSeconds = Math.floor(time / 1000);
        let remaining = totalSeconds;

        const countdownInterval = setInterval(() => {
          countdownDiv.textContent = `‚è±Ô∏è Recording: ${remaining}s remaining`;
          remaining--;
        }, 1000);

        await sleep(time)

        clearInterval(countdownInterval);
        countdownDiv.textContent = '‚úÖ Processing audio...';

        // Stop recorder and wait for data
        await new Promise((resolveStop) => {
          recorder.onstop = resolveStop;
          recorder.stop();
        });

        // Stop all tracks to release microphone
        stream.getTracks().forEach(track => track.stop());

        // Convert to base64
        const blob = new Blob(chunks, {type: 'audio/webm'})
        const text = await b2text(blob)

        countdownDiv.remove();
        resolve(text)
      } catch (error) {
        countdownDiv.textContent = '‚ùå Error: ' + error.message;
        setTimeout(() => countdownDiv.remove(), 3000);
        resolve(null);
      }
    })
    """

    try:
        duration_ms = duration_seconds * 1000
        print(f"\n‚è∫Ô∏è  Recording... Speak now!")
        print(f"‚è±Ô∏è  Recording for {duration_seconds} seconds\n")
        print("üí° Watch for the countdown timer!\n")

        # Record for specified duration
        display(Javascript(RECORD))
        audio_data = eval_js(f'record({duration_ms})')

        if not audio_data:
            print("‚ùå No audio data captured.")
            return None

        # Decode and save the audio
        audio_bytes = b64decode(audio_data.split(',')[1])
        audio_path = '/content/recorded_audio.webm'

        with open(audio_path, 'wb') as f:
            f.write(audio_bytes)

        print("‚úÖ Recording complete!")
        print(f"üíæ Saved to: {audio_path}")
        print(f"üìè Duration: {duration_seconds} seconds")
        return audio_path

    except Exception as e:
        print(f"‚ùå Error during recording: {e}")
        return None

def transcribe_audio(audio_path):
    """
    Transcribes speech to text using OpenAI Whisper.

    Args:
        audio_path (str): Path to the audio file

    Returns:
        str: Transcribed text or None if error
    """
    if not client:
        print("‚ùå OpenAI client not initialized. Cannot transcribe audio.")
        return None

    try:
        with open(audio_path, "rb") as audio_file:
            transcript = client.audio.transcriptions.create(
                model="whisper-1",
                file=audio_file
            )
        print("‚úÖ Audio transcription complete.")
        return transcript.text

    except FileNotFoundError:
        print(f"‚ùå Error: Audio file not found at {audio_path}")
        return None
    except Exception as e:
        print(f"‚ùå Error transcribing audio: {e}")
        return None

def generate_ai_response(question, transcript_text):
    """
    Analyzes a transcribed answer for emotions, themes, and personal values.

    Args:
        question (str): The question that was asked
        transcript_text (str): The transcribed answer

    Returns:
        str: AI-generated summary or None if error
    """
    if not client:
        print("‚ùå OpenAI client not initialized. Cannot generate AI response.")
        return None

    try:
        response = client.chat.completions.create(
            model="gpt-4o-mini",
            messages=[
                {
                    "role": "system",
                    "content": "You are an empathetic conversational AI analyzing life stories to extract meaning, emotion, and values."
                },
                {
                    "role": "user",
                    "content": (
                        f"Question: {question}\n"
                        f"Answer: {transcript_text}\n\n"
                        "Summarize the main emotions, themes, and values expressed in this answer."
                    )
                }
            ],
            temperature=0.7
        )
        summary = response.choices[0].message.content.strip()
        print("‚úÖ AI analysis complete.")
        return summary

    except Exception as e:
        print(f"‚ùå Error generating AI response: {e}")
        return None

"""## 5. Main Pipeline with Voice

Run this cell to execute the full pipeline with spoken questions!
"""

def run_pipeline_with_voice():
    """
    Main pipeline with Text-to-Speech for questions AND AI responses.
    """
    print("üöÄ Starting Life Review AI Pipeline with Voice...\n")
    print("=" * 50)

    # Check if client is initialized
    if not client:
        print("\n‚ùå Cannot proceed without OpenAI client. Please initialize it first.")
        return

    # === STEP 1: Upload and extract questions from PDF ===
    print("\nüì§ STEP 1: Upload PDF with life review questions")
    print("-" * 50)
    uploaded_pdf = files.upload()

    if not uploaded_pdf:
        print("‚ùå No PDF uploaded. Exiting.")
        return

    pdf_filename = list(uploaded_pdf.keys())[0]
    pdf_path = f"/content/{pdf_filename}"

    questions = extract_questions_from_pdf(pdf_path)

    if not questions:
        print("\n‚ùå No questions found. Exiting.")
        return

    print(f"\nüìò Questions extracted (showing first 5):")
    for i, q in enumerate(questions[:5], 1):
        print(f"  {i}. {q}")

    if len(questions) > 5:
        print(f"  ... and {len(questions) - 5} more")

    # === STEP 2: Configure preferences ONCE ===
    print("\n" + "=" * 50)
    print("\n‚öôÔ∏è STEP 2: Configure Preferences")
    print("-" * 50)

    # Voice preference
    print("Available voices: alloy, echo, fable, onyx, nova, shimmer")
    voice_choice = input("Choose a voice (or press Enter for 'nova'): ").strip().lower()
    voice = voice_choice if voice_choice in ['alloy', 'echo', 'fable', 'onyx', 'nova', 'shimmer'] else 'nova'
    print(f"‚úÖ Using voice: {voice}")

    # Recording duration preference
    duration_input = input("\nDefault recording duration in seconds (press Enter for 30): ").strip()
    default_duration = int(duration_input) if duration_input.isdigit() else 30
    print(f"‚úÖ Recording duration set to: {default_duration} seconds")

    # Speak AI responses preference
    speak_analysis_input = input("\nShould I speak my analysis out loud? (y/n, default yes): ").strip().lower()
    speak_analysis = speak_analysis_input != 'n'
    print(f"‚úÖ Will {'speak' if speak_analysis else 'show text for'} AI analysis")

    # Number of questions
    num_questions_input = input(f"\nHow many questions to ask? (1-{len(questions)}, or 'all'): ").strip()
    if num_questions_input.lower() == 'all':
        num_questions = len(questions)
    else:
        try:
            num_questions = min(int(num_questions_input), len(questions))
        except:
            num_questions = 1

    print(f"‚úÖ Will ask {num_questions} question(s)")

    # === STEP 3: Interview Loop ===
    print("\n" + "=" * 50)
    print("\nüé§ STEP 3: Interactive Interview")
    print("-" * 50)
    print("Starting interview in 3 seconds...\n")
    time.sleep(3)

    # Storage for responses
    all_responses = []

    # Process each question
    for i in range(num_questions):
        question = questions[i]
        print("\n" + "=" * 50)
        print(f"\nüìù Question {i+1} of {num_questions}")
        print("-" * 50)
        print(f"Question: {question}\n")

        # Speak the question
        speak_text(question, voice=voice)

        # Wait for speech to finish playing
        time.sleep(3)

        # Record answer automatically
        print(f"\nüéôÔ∏è Get ready to answer...")
        time.sleep(1)

        audio_path = record_audio(duration_seconds=default_duration)
        transcript_text = None

        if audio_path:
            transcript_text = transcribe_audio(audio_path)
            if transcript_text:
                print(f"\nüó£Ô∏è Your Answer:")
                print("-" * 50)
                print(transcript_text)

        # Generate and speak AI analysis
        if transcript_text:
            print("\nüß† Analyzing your response...")
            ai_summary = generate_ai_response(question, transcript_text)

            if ai_summary:
                print("\nüìä AI Analysis:")
                print("-" * 50)
                print(ai_summary)

                # Speak the analysis if enabled
                if speak_analysis:
                    print("\nüîä Speaking analysis...")
                    time.sleep(1)
                    speak_text(ai_summary, voice=voice)
                    time.sleep(2)

                # Store the response
                all_responses.append({
                    'question': question,
                    'answer': transcript_text,
                    'analysis': ai_summary
                })
        else:
            print("\n‚ö†Ô∏è No answer recorded. Skipping analysis.")

        # Ask if user wants to continue (only if not the last question)
        if i < num_questions - 1:
            print("\n" + "-" * 50)
            continue_choice = input("Continue to next question? (y/n): ").strip().lower()
            if continue_choice != 'y':
                print("\n‚èπÔ∏è Stopping interview...")
                break

    # === STEP 4: Summary ===
    print("\n" + "=" * 50)
    print("\nüìä Interview Summary")
    print("=" * 50)
    print(f"\n‚úÖ Completed {len(all_responses)} question(s)")

    if all_responses:
        print("\nüìù Your responses have been saved and analyzed!")
        print("\nYou can review them above or export them if needed.")

    print("\n" + "=" * 50)
    print("‚úÖ Pipeline complete!")

    return all_responses

# Run the pipeline
responses = run_pipeline_with_voice()

"""## 6. Test TTS Voices

Want to hear the different voice options? Run this cell:
"""

# Test different TTS voices
test_text = "Hello! This is what I sound like. What do you think?"

voices = ['alloy', 'echo', 'fable', 'onyx', 'nova', 'shimmer']

print("üéµ Testing all available voices...\n")

for voice in voices:
    print(f"\nüîä Voice: {voice}")
    speak_text(test_text, voice=voice)
    time.sleep(3)  # Wait between voices

"""## 7. Quick Single Question Test

Test the full cycle with just one question:
"""

# Quick test with a single question
test_question = "What is your favorite childhood memory?"

print("üéØ Quick Test: Single Question Interview\n")
print(f"Question: {test_question}\n")

# Speak the question
speak_text(test_question)
time.sleep(2)

# Record answer
print("\nüìù Now record your answer...")
duration = int(input("Recording duration (seconds): ") or "20")
audio_path = record_audio(duration_seconds=duration)

if audio_path:
    # Transcribe
    transcript = transcribe_audio(audio_path)

    if transcript:
        print(f"\nüó£Ô∏è Your Answer:\n{transcript}")

        # Analyze
        analysis = generate_ai_response(test_question, transcript)
        if analysis:
            print(f"\nüìä Analysis:\n{analysis}")